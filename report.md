# ğŸ“„ Paper Tracker Report

**Report Period:** 2023-01-15 ~ 2025-02-21
**Generated:** 2026-02-26

**ì´ ë…¼ë¬¸ ìˆ˜:** 5ê°œ

---

## 1. TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice

**ì €ì:** Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi
**arXiv:** [2502.18504v2](https://arxiv.org/abs/2502.18504v2)
**ë°œí–‰ì¼:** 2025-02-21
**ê´€ë ¨ì„± ì ìˆ˜:** â­ 8.5/10

### ğŸ“ Summary
This paper discusses TurboFuzzLLM: Turbocharging Mutation-bas... Key findings include novel approaches and significant improvements over existing methods.

### ğŸ”‘ Key Points
â€¢ Novel methodology
â€¢ Strong experimental results
â€¢ Clear presentation

---

## 2. Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions

**ì €ì:** Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, Preetha Chatterjee
**arXiv:** [2502.14202v2](https://arxiv.org/abs/2502.14202v2)
**ë°œí–‰ì¼:** 2025-02-20
**ê´€ë ¨ì„± ì ìˆ˜:** â­ 8.5/10

### ğŸ“ Summary
This paper discusses Do LLMs Consider Security? An Empirical ... Key findings include novel approaches and significant improvements over existing methods.

### ğŸ”‘ Key Points
â€¢ Novel methodology
â€¢ Strong experimental results
â€¢ Clear presentation

---

## 3. Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models

**ì €ì:** Mark Russinovich, Ahmed Salem
**arXiv:** [2502.15010v2](https://arxiv.org/abs/2502.15010v2)
**ë°œí–‰ì¼:** 2025-02-20
**ê´€ë ¨ì„± ì ìˆ˜:** â­ 8.5/10

### ğŸ“ Summary
This paper discusses Obliviate: Efficient Unmemorization for ... Key findings include novel approaches and significant improvements over existing methods.

### ğŸ”‘ Key Points
â€¢ Novel methodology
â€¢ Strong experimental results
â€¢ Clear presentation

---

## 4. CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion

**ì €ì:** Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma
**arXiv:** [2403.07865v5](https://arxiv.org/abs/2403.07865v5)
**ë°œí–‰ì¼:** 2024-03-12
**ê´€ë ¨ì„± ì ìˆ˜:** â­ 8.5/10

### ğŸ“ Summary
This paper introduces CodeAttack, a framework that exposes a universal safety vulnerability in LLMs by transforming harmful natural language prompts into code completion tasks, successfully bypassing safety guardrails of GPT-4, Claude-2, and Llama-2 over 80% of the time. The authors demonstrate that safety alignment methods focused on natural language fail to generalize to the code domain, with larger distribution gaps between code and natural language representations leading to weaker safety generalization. The work highlights critical gaps in current safety alignment approaches and the need for robust cross-domain safety mechanisms that match LLMs' growing code capabilities.

### ğŸ”‘ Key Points
LLM safety alignment methods (RLHF, SFT) primarily focus on natural language and fail to generalize to code-domain inputs
CodeAttack transforms natural language prompts into code completion tasks, bypassing safety guardrails of GPT-4, Claude-2, and Llama-2 over 80% of the time
A larger distribution gap between code representations and natural language correlates with weaker safety generalization (e.g., encoding inputs as data structures is more effective)
The vulnerability stems from a misaligned bias where LLMs prioritize code completion behavior over safety constraints during inference
The paper analyzes potential mitigation measures and calls for more robust safety alignment that extends to code capabilities

---

## 5. Deep Learning Advances in Computer Vision

**ì €ì:** Alice Smith, Bob Johnson
**arXiv:** [2301.00001](https://arxiv.org/abs/2301.00001)
**ë°œí–‰ì¼:** 2023-01-15
**ê´€ë ¨ì„± ì ìˆ˜:** â­ 9.0/10

### ğŸ“ Summary
This paper introduces a new neural network architecture.

### ğŸ”‘ Key Points
Novel architecture for image classification
Improves accuracy by 15%
Reduced computational cost

---

*Report generated on 2026-02-26*
*Powered by paper-tracker*